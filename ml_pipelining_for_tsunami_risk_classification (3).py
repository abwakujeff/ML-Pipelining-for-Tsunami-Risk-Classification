# -*- coding: utf-8 -*-
"""ML Pipelining for Tsunami Risk Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwXAlM7vrtGmbAwxlQ9xImZC3zFoEJpY
"""

# Install required packages
!pip install lazypredict shap

# Core data handling
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn for ML pipeline
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,
                            classification_report, accuracy_score, f1_score)

# Model evaluation and selection
from lazypredict.Supervised import LazyClassifier

# Model interpretation
import shap

# Utilities
import warnings
warnings.filterwarnings('ignore')

# Set better plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
print("All packages imported successfully!")

# Load the dataset
PATH = '/content/earthquake_data_tsunami.csv'
RECORDS = pd.read_csv(PATH)

# Display basic information about the dataset
print("Dataset Shape:", RECORDS.shape)
print("\nFirst 5 rows:")
RECORDS.head()

# Get more detailed information about the dataset
print("Dataset Info:")
print(RECORDS.info())

# Check for missing values
print("Missing Values:")
missing_data = RECORDS.isnull().sum()
print(missing_data[missing_data > 0])

# Basic statistical summary
print("Statistical Summary:")
RECORDS.describe()

# Check the target variable distribution (assuming it's related to tsunami risk)
# Let's see what columns we have
print("Column Names:")
print(RECORDS.columns.tolist())

"""###Data Insights & Observations:

###1. Target Variable Analysis:
"""

# Let's check the target distribution
tsunami_counts = RECORDS['tsunami'].value_counts()
tsunami_percentage = RECORDS['tsunami'].value_counts(normalize=True) * 100

print("Target Variable Distribution:")
print(f"Non-Tsunami Events (0): {tsunami_counts[0]} ({tsunami_percentage[0]:.1f}%)")
print(f"Tsunami Events (1): {tsunami_counts[1]} ({tsunami_percentage[1]:.1f}%)")

"""Insight: We have a reasonably balanced dataset (61% vs 39%) which is good for binary classification.

###2. Key Feature Correlations:
"""

# Correlation with target variable
correlation_with_target = RECORDS.corr()['tsunami'].sort_values(ascending=False)
print("\nFeature correlations with tsunami risk:")
print(correlation_with_target)

