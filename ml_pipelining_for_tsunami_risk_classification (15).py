# -*- coding: utf-8 -*-
"""ML Pipelining for Tsunami Risk Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwXAlM7vrtGmbAwxlQ9xImZC3zFoEJpY
"""

# Install required packages
!pip install lazypredict shap

# Core data handling
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn for ML pipeline
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,
                            classification_report, accuracy_score, f1_score)

# Model evaluation and selection
from lazypredict.Supervised import LazyClassifier

# Model interpretation
import shap

# Utilities
import warnings
warnings.filterwarnings('ignore')

# Set better plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
print("All packages imported successfully!")

# Load the dataset
PATH = '/content/earthquake_data_tsunami.csv'
RECORDS = pd.read_csv(PATH)

# Display basic information about the dataset
print("Dataset Shape:", RECORDS.shape)
print("\nFirst 5 rows:")
RECORDS.head()

# Get more detailed information about the dataset
print("Dataset Info:")
print(RECORDS.info())

# Check for missing values
print("Missing Values:")
missing_data = RECORDS.isnull().sum()
print(missing_data[missing_data > 0])

# Basic statistical summary
print("Statistical Summary:")
RECORDS.describe()

# Check the target variable distribution (assuming it's related to tsunami risk)
# Let's see what columns we have
print("Column Names:")
print(RECORDS.columns.tolist())

"""###Data Insights & Observations:

###1. Target Variable Analysis:
"""

# Let's check the target distribution
tsunami_counts = RECORDS['tsunami'].value_counts()
tsunami_percentage = RECORDS['tsunami'].value_counts(normalize=True) * 100

print("Target Variable Distribution:")
print(f"Non-Tsunami Events (0): {tsunami_counts[0]} ({tsunami_percentage[0]:.1f}%)")
print(f"Tsunami Events (1): {tsunami_counts[1]} ({tsunami_percentage[1]:.1f}%)")

"""Insight: We have a reasonably balanced dataset (61% vs 39%) which is good for binary classification.

###2. Key Feature Correlations:
"""

# Correlation with target variable
correlation_with_target = RECORDS.corr()['tsunami'].sort_values(ascending=False)
print("\nFeature correlations with tsunami risk:")
print(correlation_with_target)

"""### 3. Critical Seismic Features
- **Magnitude:** Range **6.5–9.1** (includes major events like *2004* & *2011* mega-earthquakes)  
- **Depth:** **2.7–670.8 km** (*shallow earthquakes = higher tsunami risk*)  
- **Location:** Global coverage with **ocean proximity indicators** (latitude/longitude)  
- **Intensity Measures:** **CDI (0–9)** and **MMI (1–9)** for impact assessment  

---

### 4. Data Quality Strengths
✅ **No missing values** – perfect for immediate modeling  
✅ **Good balance between classes**  
✅ **Global coverage** across *22 years*  
✅ **Clean feature types** (all numerical)  

---

### 5. Potential Feature Engineering Opportunities
- Derive **ocean proximity** from latitude/longitude  
- Extract **seasonal patterns** from the *Month* feature  
- Identify **temporal trends** from the *Year* feature  
- Create **depth categories** (*shallow vs deep earthquakes*)  

---

### 6. ML Pipeline Strategy
Based on the data, we should:

1. **Focus** on magnitude, depth, and geographic features as primary predictors  
2. **Apply feature scaling** due to differing ranges (magnitude vs depth vs coordinates)  
3. **Use stratified sampling** to maintain class balance  
4. **Start with tree-based models** that handle feature importance effectively  

"""

# 1. Target variable distribution visualization
plt.figure(figsize=(15, 10))

# Target distribution
plt.subplot(2, 3, 1)
tsunami_counts = RECORDS['tsunami'].value_counts()
plt.pie(tsunami_counts.values, labels=['No Tsunami', 'Tsunami'], autopct='%1.1f%%',
        colors=['lightcoral', 'lightblue'], startangle=90)
plt.title('Tsunami Event Distribution')

# 1. Target variable distribution visualization
plt.figure(figsize=(15, 10))

# Target distribution
plt.subplot(2, 3, 1)
tsunami_counts = RECORDS['tsunami'].value_counts()
plt.pie(tsunami_counts.values, labels=['No Tsunami', 'Tsunami'], autopct='%1.1f%%',
        colors=['lightcoral', 'lightblue'], startangle=90)
plt.title('Tsunami Event Distribution')

# 2. Magnitude distribution by tsunami risk
plt.subplot(2, 3, 2)
sns.boxplot(x='tsunami', y='magnitude', data=RECORDS, palette=['lightcoral', 'lightblue'])
plt.title('Magnitude Distribution by Tsunami Risk')
plt.xlabel('Tsunami (0=No, 1=Yes)')
plt.ylabel('Magnitude')

# 3. Depth distribution by tsunami risk
plt.subplot(2, 3, 3)
sns.boxplot(x='tsunami', y='depth', data=RECORDS, palette=['lightcoral', 'lightblue'])
plt.title('Depth Distribution by Tsunami Risk')
plt.xlabel('Tsunami (0=No, 1=Yes)')
plt.ylabel('Depth (km)')

# 4. Correlation heatmap
plt.subplot(2, 3, 4)
correlation_matrix = RECORDS.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            fmt='.2f', linewidths=0.5)
plt.title('Feature Correlation Heatmap')
plt.xticks(rotation=45)
plt.yticks(rotation=0)

# 5. Geographic distribution of tsunami events
plt.subplot(2, 3, 5)
tsunami_events = RECORDS[RECORDS['tsunami'] == 1]
non_tsunami_events = RECORDS[RECORDS['tsunami'] == 0]
plt.scatter(non_tsunami_events['longitude'], non_tsunami_events['latitude'],
            alpha=0.6, c='red', label='No Tsunami', s=20)
plt.scatter(tsunami_events['longitude'], tsunami_events['latitude'],
            alpha=0.6, c='blue', label='Tsunami', s=20)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Geographic Distribution of Tsunami Events')
plt.legend()

# 6. Feature importance preview (correlation with target)
plt.subplot(2, 3, 6)
target_correlations = correlation_matrix['tsunami'].drop('tsunami').sort_values()
colors = ['red' if x < 0 else 'blue' for x in target_correlations]
plt.barh(range(len(target_correlations)), target_correlations.values, color=colors)
plt.yticks(range(len(target_correlations)), target_correlations.index)
plt.title('Feature Correlation with Tsunami Risk')
plt.xlabel('Correlation Coefficient')

plt.tight_layout()
plt.show()

# 7. Statistical summary by tsunami class
print("Statistical Summary - Tsunami vs Non-Tsunami Events:")
print("=" * 50)

tsunami_stats = RECORDS.groupby('tsunami').agg({
    'magnitude': ['mean', 'std', 'min', 'max'],
    'depth': ['mean', 'std', 'min', 'max'],
    'sig': ['mean', 'std'],
    'cdi': ['mean', 'std']
}).round(2)

print(tsunami_stats)

# 8. Check for temporal patterns
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
yearly_tsunami = RECORDS.groupby('Year')['tsunami'].mean()
plt.plot(yearly_tsunami.index, yearly_tsunami.values, marker='o', linewidth=2)
plt.title('Tsunami Frequency by Year')
plt.xlabel('Year')
plt.ylabel('Tsunami Proportion')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
monthly_tsunami = RECORDS.groupby('Month')['tsunami'].mean()
plt.bar(monthly_tsunami.index, monthly_tsunami.values, color='skyblue', alpha=0.7)
plt.title('Tsunami Frequency by Month')
plt.xlabel('Month')
plt.ylabel('Tsunami Proportion')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""# 1. Prepare features and target"""

X = RECORDS.drop('tsunami', axis=1)
y = RECORDS['tsunami']

print("Feature set shape:", X.shape)
print("Target set shape:", y.shape)

# 2. Split the data with stratification to maintain class balance
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # Important for imbalanced datasets
)

print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)
print("\nClass distribution in training set:")
print(y_train.value_counts(normalize=True))
print("\nClass distribution in testing set:")
print(y_test.value_counts(normalize=True))

# 3. Create preprocessing pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# Define which columns to scale (all numerical features)
numeric_features = X.columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features)
    ])

print("Preprocessing pipeline created successfully!")
print("Features to be scaled:", numeric_features)

# 4. Quick model comparison using LazyClassifier
print("Running quick model comparison with LazyClassifier...")

# Note: This might take a few minutes
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

# Display top 10 models
print("\nTop 10 Performing Models:")
print(models.head(10))

# 5. Let's also check the performance of a few key models manually
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Initialize models
models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(random_state=42),
    "SVM": SVC(random_state=42)
}

# Evaluate each model
print("Manual Model Evaluation:")
print("=" * 50)

for name, model in models.items():
    # Create pipeline with preprocessing and model
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])

    # Train and predict
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)

    print(f"\n{name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(classification_report(y_test, y_pred))

